---
title: "MTA Ridership Project"
output: html_notebook
---

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(splines)
library(glmnet)
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)
library(ggplot2)

options(scipen = 999)
```


```{r}

url <- "https://data.ny.gov/api/views/vxuj-8kew/rows.csv?accessType=DOWNLOAD"
df<- read_csv(url)
# df <- read_csv("MTA_Daily_Ridership_Data.csv")
glimpse(df)
colnames(df)

```

## Preparing Data: Regression Section
```{r}
response_var <- "Subways: Total Estimated Ridership"
predictor_vars <- c("Buses: Total Estimated Ridership", 
                    "LIRR: Total Estimated Ridership", 
                    "Metro-North: Total Estimated Ridership",
                    "Access-A-Ride: Total Scheduled Trips", 
                    "Bridges and Tunnels: Total Traffic", 
                    "Staten Island Railway: Total Estimated Ridership")

df_clean <- na.omit(df[, c(response_var, predictor_vars)])

# Create matrices
x <- as.matrix(df_clean[, predictor_vars])
y <- df_clean[[response_var]]

# Train/Test Split
set.seed(168)
train_indices <- sample(1:nrow(x), 0.8 * nrow(x))
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test  <- x[-train_indices, ]
y_test  <- y[-train_indices]

```
## Ridge and Lasso Regression
```{r}
# Ridge
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)
ridge_pred <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)
ridge_mse <- mean((ridge_pred - y_test)^2)

# Lasso
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)
lasso_pred <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)
lasso_mse <- mean((lasso_pred - y_test)^2)

cat("Ridge MSE:", ridge_mse, "\n")
cat("Lasso MSE:", lasso_mse, "\n")

```

## Ridge vs Lasso: Predicted vs Actual Plot
```{r}
# Plotting
ridge_plot_df <- data.frame(Actual = y_test, Predicted = as.numeric(ridge_pred), Model = "Ridge")
lasso_plot_df <- data.frame(Actual = y_test, Predicted = as.numeric(lasso_pred), Model = "Lasso")
all_predictions_df <- rbind(ridge_plot_df, lasso_plot_df)

ggplot(all_predictions_df, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(
    title = "Predicted vs Actual Subway Ridership",
    x = "Actual Ridership",
    y = "Predicted Ridership",
    color = "Model"
  ) +
  theme_minimal()

```

```{r}
# This predicted vs actual subway ridership plot indicates that our Ridge and Lasso regression models are performing reasonably well in predicting subway ridership based on the other transportation metrics.

# Comparing our MSE, we can see that the Lasso regression model has a slightly lower Mean Squared Error than the Ridge regression model on your test data. A lower MSE indicates that, on average, the predictions made by the Lasso model are closer to the actual subway ridership values in the test set compared to the Ridge model. The difference in MSE, while present, might not be drastically large. It suggests that Lasso has a marginal improvement in predictive accuracy for this specific dataset and the chosen model parameters.
```


## Ridge/Lasso CV Plots
```{r}
# Create data frames for ridge and lasso
ridge_df <- data.frame(
  log_lambda = log(ridge_model$lambda),
  mse_mean = ridge_model$cvm,
  mse_upper = ridge_model$cvup,
  mse_lower = ridge_model$cvlo
)

lasso_df <- data.frame(
  log_lambda = log(lasso_model$lambda),
  mse_mean = lasso_model$cvm,
  mse_upper = lasso_model$cvup,
  mse_lower = lasso_model$cvlo
)

ridge_df <- ridge_df %>% mutate(across(c(mse_mean, mse_upper, mse_lower), ~ .x / 1e9))
lasso_df <- lasso_df %>% mutate(across(c(mse_mean, mse_upper, mse_lower), ~ .x / 1e9))

# Ridge plot
ridge_plot <- ggplot(ridge_df, aes(x = log_lambda, y = mse_mean)) +
  geom_point(color = "red", size = 2) +
  geom_errorbar(aes(ymin = mse_lower, ymax = mse_upper), width = 0.05, color = "gray") +
  labs(title = "Ridge: CV MSE vs Lambda", 
       x = expression(log(lambda)), 
       y = "Mean-Squared Error (Billions)") +
  scale_y_continuous(labels = scales::comma) +  # <-- nice labels: 0, 200, 400
  theme_minimal(base_size = 14) # slightly larger base font

# Lasso plot
lasso_plot <- ggplot(lasso_df, aes(x = log_lambda, y = mse_mean)) +
  geom_point(color = "red", size = 2) +
  geom_errorbar(aes(ymin = mse_lower, ymax = mse_upper), width = 0.05, color = "gray") +
  labs(title = "Lasso: CV MSE vs Lambda", 
       x = expression(log(lambda)), 
       y = "Mean-Squared Error (Billions)") +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal(base_size = 14)

grid.arrange(ridge_plot, lasso_plot, ncol = 2)


```

```{r}
"Ridge: 'Hockey stick' MSE curve vs. log(λ), low MSE at low λ (overfitting), increases with regularization (underfitting), plateaus at high λ. Optimal λ suggests moderate regularization prevents overfitting. Small error bars indicate consistent CV performance. Minimum MSE in lower tens of billions.

Lasso: MSE increases with log(λ) from minimum, potentially steeper than Ridge. Optimal λ lower, suggesting less regularization needed. Small error bars. Minimum MSE in lower tens of billions, possibly slightly better than Ridge.

Comparison: Similar minimum MSE for both (billions indicate substantial unexplained variance). Lasso prefers less regularization, implying potential feature selection of less influential predictors, offering insight into key factors. Both handle multicollinearity. Lasso's slight edge hints at removing less relevant variables."
```


## Classification: Decision Tree and Random Forest

```{r}
# Creating categorical target variable
quantiles <- quantile(df_clean[[response_var]], probs = c(0.33, 0.66))
df_clean$ridership_class <- cut(df_clean[[response_var]],
                                breaks = c(-Inf, quantiles, Inf),
                                labels = c("Low", "Medium", "High"),
                                right = TRUE)

# Class distribution
table(df_clean$ridership_class)


```

```{r}
# Train/Test split for classification
set.seed(168)
train_index <- createDataPartition(df_clean$ridership_class, p = 0.8, list = FALSE)

train_data <- df_clean[train_index, ]
test_data  <- df_clean[-train_index, ]

# Clean column names (no colons/spaces)
train_data_clean <- train_data %>%
  rename_with(~ gsub("[^[:alnum:]_]", "_", .))

test_data_clean <- test_data %>%
  rename_with(~ gsub("[^[:alnum:]_]", "_", .))

```

## Training Decision Tree
```{r}
tree_model <- rpart(ridership_class ~ Buses__Total_Estimated_Ridership +
                                    LIRR__Total_Estimated_Ridership +
                                    Metro_North__Total_Estimated_Ridership +
                                    Access_A_Ride__Total_Scheduled_Trips +
                                    Bridges_and_Tunnels__Total_Traffic +
                                    Staten_Island_Railway__Total_Estimated_Ridership,
                    data = train_data_clean,
                    method = "class",
                    control = rpart.control(maxdepth = 5, minsplit = 30))

# Checking the complexity parameter to prune
printcp(tree_model)


# Pruning
pruned_tree <- prune(tree_model, cp = tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"])

rpart.plot(pruned_tree, type = 2, extra = 104, fallen.leaves = TRUE)


```


```{r}
"The decision tree classifies daily subway ridership (low, average, high) using other MTA services' ridership. LIRR, Metro-North, Access-A-Ride, and Staten Island Railway ridership were key predictors, suggesting interconnected demand. Surprisingly, bus ridership and bridge/tunnel traffic were not significant in this model. The initial guessing error was high (66%), highlighting the model's value. By pruning the tree using cross-validation error, we aim for a model that accurately classifies future subway ridership based on these relationships. The tree's rules offer insights into how demand across different transit modes interacts, which can inform future, integrated transportation policies and service planning."
```


## Train Random Forest
```{r}
rf_model <- randomForest(ridership_class ~ Buses__Total_Estimated_Ridership +
                                          LIRR__Total_Estimated_Ridership +
                                          Metro_North__Total_Estimated_Ridership +
                                          Access_A_Ride__Total_Scheduled_Trips +
                                          Bridges_and_Tunnels__Total_Traffic +
                                          Staten_Island_Railway__Total_Estimated_Ridership,
                         data = train_data_clean,
                         ntree = 500,
                         importance = TRUE)

print(rf_model)

```

## Evaluating Tree and RF
```{r}
tree_pred <- predict(tree_model, test_data_clean, type = "class")
rf_pred   <- predict(rf_model, test_data_clean)

# Confusion Matrices
cat("Decision Tree Confusion Matrix:\n")
print(confusionMatrix(tree_pred, test_data_clean$ridership_class))

cat("\nRandom Forest Confusion Matrix:\n")
print(confusionMatrix(rf_pred, test_data_clean$ridership_class))

```


```{r}
"Overall Accuracy: Random Forest achieves a higher accuracy (93.22%) compared to the Decision Tree (89.27%), indicating a greater ability to correctly classify ridership levels.

Kappa Statistic: Random Forest has a substantially higher Kappa (0.8983 vs. 0.839), suggesting better agreement between predicted and actual classifications beyond chance.

Sensitivity (Recall): While the sensitivity for the 'Low' class is similar for both, Random Forest shows higher sensitivity for 'Medium' (91.45% vs. 85.47%) and 'High' (93.33% vs. 87.50%) ridership, meaning it's better at correctly identifying these categories.

Specificity: Random Forest exhibits higher specificity across all classes, particularly for 'Low' (98.73% vs. 95.36%) and 'Medium' (94.09% vs. 91.14%), indicating a better ability to correctly identify days that do not belong to each ridership level.

Positive Predictive Value (Precision): Random Forest has a higher precision for 'Low' (97.37% vs. 90.98%) and 'Medium' (88.43% vs. 82.64%) ridership, meaning when it predicts these levels, it is more likely to be correct. The precision for 'High' is comparable.

Balanced Accuracy: Random Forest demonstrates higher balanced accuracy across all classes, especially for 'Medium' (92.77% vs. 88.30%) and 'High' (95.17% vs. 92.47%), indicating better performance when class imbalance is considered."
```



## Variable Importance (Random Forest)
```{r}
importance(rf_model)

```
```{r}
rf_importance <- importance(rf_model)
rf_importance_df <- data.frame(Feature = rownames(rf_importance), Importance = rf_importance[,1])
rf_importance_df <- rf_importance_df[order(-rf_importance_df$Importance),]

ggplot(rf_importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Random Forest Feature Importance", x = "Feature", y = "Importance") +
  theme_minimal()

```


## Accuracy
```{r}
tree_accuracy <- sum(tree_pred == test_data_clean$ridership_class) / length(tree_pred)
rf_accuracy <- sum(rf_pred == test_data_clean$ridership_class) / length(rf_pred)

cat(sprintf("Decision Tree Accuracy: %.2f%%\n", tree_accuracy * 100))
cat(sprintf("Random Forest Accuracy: %.2f%%\n", rf_accuracy * 100))

```

## Cross-Validation: K-Fold CV for Decision Trees
```{r}
set.seed(168)
train_control <- trainControl(method = "cv", number = 10)

# Train CV Tree model
tree_cv_model <- train(ridership_class ~ Buses__Total_Estimated_Ridership +
                                    LIRR__Total_Estimated_Ridership +
                                    Metro_North__Total_Estimated_Ridership +
                                    Access_A_Ride__Total_Scheduled_Trips +
                                    Bridges_and_Tunnels__Total_Traffic +
                                    Staten_Island_Railway__Total_Estimated_Ridership,
                       data = train_data_clean,
                       method = "rpart",
                       trControl = train_control)

# CV Model Summary
print(tree_cv_model)

# Predict using CV model
tree_pred_cv <- predict(tree_cv_model, test_data_clean)

# Confusion Matrix
cat("Decision Tree (CV) Confusion Matrix:\n")
print(confusionMatrix(tree_pred_cv, test_data_clean$ridership_class))

```
```{r}
"Accuracy: Random Forest (93.22%) still significantly outperforms the cross-validated Decision Tree (87.57%).

Kappa: Random Forest (0.8983) shows substantially better agreement than the cross-validated Decision Tree (0.8136).

Sensitivity: Random Forest generally maintains higher or comparable sensitivity across the classes, 'particularly for 'Medium' and 'High' ridership.

Specificity: Random Forest consistently shows higher specificity, indicating a better ability to correctly identify days not belonging to each class.

Balanced Accuracy: Random Forest exhibits higher balanced accuracy across all classes, suggesting more robust performance when considering potential class imbalance."
```



```{r}
" Over all, The strong performance of the Random Forest model highlights just how many different factors influence subway ridership. Both models show that commuter rail services like the LIRR and Metro-North play a major role, even though each model weighs their importance a little differently. This points to a strong connection between regional commuting patterns and how people use the subway.

Access-A-Ride also showed up as a key factor, suggesting that paratransit demand might be linked to specific levels of subway ridership, maybe reflecting the needs of certain groups of riders who rely more heavily on these services.

On the other hand, bus ridership and bridge/tunnel traffic were less important in the Random Forest model, and they didn’t even appear in the decision tree structure. This suggests that while buses and car traffic might affect transit patterns in general, they’re not the strongest indicators when it comes to classifying daily subway ridership as 'low,' 'average,' or 'high.' That said, they could still be valuable for predicting the actual number of riders (as we saw in the regression models) or for understanding more localized transit behaviors.

The Random Forest's high accuracy rate (93.22%) gives us a lot of confidence in its ability to predict daily ridership categories moving forward. This could be incredibly useful for the MTA when it comes to planning ahead, such as adjusting train schedules, staffing, and resource allocation based on expected rider volumes. For example, if we can predict a 'high' ridership day ahead of time based on trends in commuter rail or paratransit usage, the MTA could proactively add service and better manage crowding.

Finally, the insights from the feature importance analysis can help guide bigger policy decisions. Knowing which modes of transit have the strongest ties to subway ridership could support more integrated planning and investment strategies across the system. For instance, encouraging commuter rail use might have predictable ripple effects on subway demand.
```


