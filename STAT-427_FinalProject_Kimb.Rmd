---
title: "STAT-427_FinalProject_Kimb"
author: "J.Kimbrough"
date: "2025-04-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r pressure, echo=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(dbplyr)
library(readr)
library(RSQLite)
library(ggthemes)
library(ggplot2)
library(class)
library(caret)
library(glmnet)
library(ISLR2)
library(MASS)
library(boot)
library(tree)
library(randomForest)
library(splines)
library(rpart)
#library(rpart.plot)  - I don't know this library, not too sure what it is. 
library(car)
```

```{r}
directory_path <- "C:/Users/jkimb/OneDrive/Desktop/School/Classes/VII. Spring 2025 (LAST SEM BABY)/ONL_STAT-427_StatisticalMachineLearning/Projects"

setwd(directory_path)
```

```{r}
mta_data_raw <- read_csv("./data/MTA_Daily_Ridership_Data__2020_-_2025.csv", na = "missing")

view(mta_data_raw)
glimpse(mta_data_raw)
```


# ---- Cleaning Data -----

```{r}
anyNA(mta_data_raw)
any(sapply(mta_data_raw, function(x) any(x=="")))
any(sapply(mta_data_raw, function(x) any(x == -999)))
```

```{r}
df <- mta_data_raw %>%
  mutate(Date = mdy(Date)) %>%
  mutate(across(where(is.character), ~ na_if(., "missing"))) %>%
  mutate(across(where(is.numeric), ~ na_if(., -999))) %>%
  mutate(across(where(is.character), ~ na_if(., ""))) %>% 
  na.omit()

head(df)
```


# --- Exploratory Data Analysis ---

```{r}
colnames(df)
```

# Histograms
```{r}
df_long <- df %>%
   dplyr::select(`Subways: Total Estimated Ridership`,
         `Buses: Total Estimated Ridership`,
         `LIRR: Total Estimated Ridership`,
         `Metro-North: Total Estimated Ridership`,
         `Staten Island Railway: Total Estimated Ridership`) %>%
  pivot_longer(cols = everything(),
               names_to = "Transportation_Mode",
               values_to = "Ridership")

ggplot(df_long, aes(x = Ridership)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  facet_wrap(~ Transportation_Mode, scales = "free_x") +
  labs(title = "Distribution of Ridership by Transportation Mode",
       x = "Total Estimated Ridership",
       y = "Frequency") +
  theme_minimal()
```

# Plotted Relationships
```{r}

df_long <- df %>%
   dplyr::select(
    `Subways: % of Comparable Pre-Pandemic Day`,
    `Buses: Total Estimated Ridership`,
    `LIRR: Total Estimated Ridership`,
    `Metro-North: Total Estimated Ridership`,
    `Access-A-Ride: Total Scheduled Trips`,
    `Bridges and Tunnels: Total Traffic`,
    `Staten Island Railway: Total Estimated Ridership`,
    `Subways: Total Estimated Ridership`
  ) %>%
  pivot_longer(
    cols = -`Subways: Total Estimated Ridership`,
    names_to = "Other_Metric",
    values_to = "Other_Value"
  )

ggplot(df_long, aes(x = Other_Value, y = `Subways: Total Estimated Ridership`)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ Other_Metric, scales = "free", ncol = 2) + # Removed the extra + before the comment
  labs(
    title = "Subway Ridership vs. Other Transportation Metrics",
    x = "Other Transportation Metric",
    y = "Subway Total Estimated Ridership"
  ) +
  theme_minimal()
```




# Correlations
```{r}
numeric_cols <- names(df)[sapply(df, is.numeric) & names(df) != "Subways: Total Estimated Ridership"]

correlation_results <- df %>%
  dplyr::select(-Date) %>% # Ensure Date is removed
  pivot_longer(
    cols = all_of(numeric_cols),
    names_to = "Other_Metric",
    values_to = "Other_Value"
  ) %>%
  group_by(Other_Metric) %>%
  summarize(
    correlation = cor(`Subways: Total Estimated Ridership`, Other_Value, use = "pairwise.complete.obs")
  )

print(correlation_results)
```

cor(df$`Subways: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.645984
### Moderate positive correlation. As subway ridership returns to pre-pandimc levels, total subway ridership tends to increase. 

> cor(df$`Buses: Total Estimated Ridership`, df$`Subways: Total Estimated Ridership`)
[1] 0.8837609

### Strong postive correlation. Higher bus ridership seems strongly associated with higher subway ridership. 

> cor(df$`Buses: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.6396882

### Moderate positive correlation. When bus ridership returns to pre-pandemic levels, subway ridership also tends to be higher.

> cor(df$`LIRR: Total Estimated Ridership`, df$`Subways: Total Estimated Ridership`)
[1] 0.9610828

### A very strong positive correlation. LIRR ridership and subway ridership seem strongly associated with each other. 

> cor(df$`LIRR: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.4087246

### A weak, but positive correlation. As LIRR ridership returns to pre-pandemic levels, it doesn't appear to affect Subway total estimated ridership significantly.

> cor(df$`Metro-North: Total Estimated Ridership`, df$`Subways: Total Estimated Ridership`)
[1] 0.9426142

### A strong positive correlation. Metro-north ridership and subway ridership seem strongly associated with each other. 

> cor(df$`Metro-North: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.6010156

### Moderate positive correlation. As Metro-north ridership returns to pre-pandemic levels, it somewhat tracks with subway usage.

> cor(df$`Access-A-Ride: Total Scheduled Trips`, df$`Subways: Total Estimated Ridership`)
[1] 0.9088294

### A strong positive correlation. Access-A-Ride trips and subway ridership seem strongly associated with each other. 

> cor(df$`Access-A-Ride: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.7061747

### Moderately positive correlation. As Access-A-Ride usage returns to pre-pandemic levels, total subway ridership tends to increase.

> cor(df$`Bridges and Tunnels: Total Traffic`, df$`Subways: Total Estimated Ridership`)
[1] 0.7381719

### Moderately positive correlation. Higher vehicle traffic seems associated with higher subway ridership.

> cor(df$`Bridges and Tunnels: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.6586184

### Moderately positive correlation. As Bridge and Tunnel traffic returns to pre-pandemic levels, total subway ridership seems to somewhat increase.

> cor(df$`Staten Island Railway: Total Estimated Ridership`, df$`Subways: Total Estimated Ridership`)
[1] 0.9233413

### A strong positive correlation. Staten Island Railway and Subway ridership seem strongly associated with each other. 

> cor(df$`Staten Island Railway: % of Comparable Pre-Pandemic Day`, df$`Subways: Total Estimated Ridership`)
[1] 0.4692073

### A weak, but positive correlation. As SIR ridership returns to pre-pandemic levels, it doesn't appear to affect Subway total estimated ridership significantly. 



# Data Summary Table
```{r}
df_with_year <- df %>%
  mutate(Year = year(Date))

ridership_summary <- df_with_year %>%
  group_by(Year) %>%
  summarize(
    Total_Subway_Ridership = sum(`Subways: Total Estimated Ridership`, na.rm = TRUE),
    Total_Bus_Ridership = sum(`Buses: Total Estimated Ridership`, na.rm = TRUE),
    Total_LIRR_Ridership = sum(`LIRR: Total Estimated Ridership`, na.rm = TRUE),
    Total_MNR_Ridership = sum(`Metro-North: Total Estimated Ridership`, na.rm = TRUE),
    Total_SIR_Ridership = sum(`Staten Island Railway: Total Estimated Ridership`, na.rm = TRUE),
    Total_AAR_Ridership =sum(`Access-A-Ride: Total Scheduled Trips`, na.rm = TRUE),
  ) %>%
  filter(Year != 2025)


print(ridership_summary)
```


# Regression Question

## Updated - What type of regression model best predicts total daily subway ridership?

## --- Multiple Linear Regression ---

```{r}
# Subways: Total Estimated Ridership

full_model <- lm(`Subways: Total Estimated Ridership` ~ ., data = df)

summary(full_model)
```
### The model explains about 99.45% of the variability in subway ridership, which is extremely high. With an F-stat of 22,890 and a p-value < 2.2e-16, the overall model is highly statistically significant.

```{r}
plot(full_model)
```


```{r}
#NO Date, Bridges and Tunnels: % of Comparable Pre-Pandemic Day
reduced_model <- lm(`Subways: Total Estimated Ridership` ~ . -Date -`Bridges and Tunnels: % of Comparable Pre-Pandemic Day`, data = df)
summary(reduced_model)
```
### This model still explains about 99.45% of the variability in subway ridership, which is extremely high. The F-stat is still pretty high at 26,630 and a low p-value. The overall model is still statistically significant.

```{r}
plot(reduced_model)
```

```{r}
step(reduced_model)
```
### Best to not drop any variables - dropping them only increases AIC. 

```{r}
anova(full_model, reduced_model)
```
### Reduced model actually worsened statistically. So, keeping full model performs better. 

```{r}
vif_full <- vif(full_model)
vif_full
```
### Some multicollinearity was detected, especially with `LIRR Total Estimated Ridership` having the highest vif. 

```{r}
predicted_full <- predict(full_model, df)

rmse_full <- sqrt(mean((df$`Subways: Total Estimated Ridership` - predicted_full)^2))
rmse_full
```

```{r}
mse_full <- (mean((df$`Subways: Total Estimated Ridership` - predicted_full)^2))
mse_full
```
### The RMSE shows that there is an average daily prediction error of approximately 78,900 riders.

```{r}
set.seed(1234)
train_control <- trainControl(method = "cv", number = 10)

model_cv <- train(
  `Subways: Total Estimated Ridership` ~ .,
  data = df,
  method = "lm",
  trControl = train_control
)

print(model_cv)
```
### Similar to predicted RMSE for the full model, the cross-validated RMSE shows that there is an average daily prediction error of approximately 80,000 riders. The R^2 is very high, and suggests that the model explains over 99% of the variability in subway ridership. 

```{r}
options(scipen = 999)
actual <- df$`Subways: Total Estimated Ridership`
predicted <- predicted_full

plot(actual, predicted,
     xlab = "Actual Subway Ridership (Billions)",
     ylab = "Predicted Subway Ridership (Billions)",
     main = "Predicted vs. Actual Subway Ridership",
     pch = 19, col = "steelblue")
abline(a = 0, b = 1, col = "red", lwd = 2)
```

# Classification Question

## Updated - Can predict “low,” “average,” or “high” ridership by using classification statistical learning methods?


```{r}
### combining ridership 
df$total_ridership <- (df$`Subways: Total Estimated Ridership`+
                         df$`Buses: Total Estimated Ridership`+
                         df$`LIRR: Total Estimated Ridership`+
                         df$`Metro-North: Total Estimated Ridership`+
                         df$`Staten Island Railway: Total Estimated Ridership`)
```

```{r}

quantiles <- quantile(df$total_ridership, probs = c(0.33, 0.66))

df$total_ridership_level <- cut(
  df$total_ridership,
  breaks = c(-Inf, quantiles[1], quantiles[2], Inf),
  labels = c("Low", "Average", "High")
)
```

```{r}
predictors <- c(
  "Subways: % of Comparable Pre-Pandemic Day",
  "Buses: % of Comparable Pre-Pandemic Day",
  "LIRR: % of Comparable Pre-Pandemic Day",
  "Metro-North: % of Comparable Pre-Pandemic Day",
  "Access-A-Ride: % of Comparable Pre-Pandemic Day",
  "Bridges and Tunnels: % of Comparable Pre-Pandemic Day",
  "Staten Island Railway: % of Comparable Pre-Pandemic Day"
)
```



## ---- KNN ---- 
```{r}
df_knn <- na.omit(df[, c(predictors, "total_ridership_level")])

```

# Because KNN is distance-based it could be good to standardize predictors by scaling them, such as
# df_knn_scaled <- df_knn %>%
#  mutate(across(all_of(predictors), scale))


```{r}
set.seed(1234)

Z = sample(nrow(df_knn), .5*nrow(df_knn))

train = df_knn[Z, predictors]

test = df_knn[-Z, predictors]

cl = df_knn$total_ridership_level[Z]
test_cl = df_knn$total_ridership_level[-Z]

```

```{r}
Yhat <- knn(train, test, cl, k = 3)
```

```{r}
conf_matx <- table(Predicted = Yhat, Actual = test_cl)
conf_matx 
```

```{r}
accuracy <- sum(diag(conf_matx)) / sum(conf_matx)
accuracy
```

### Errors mostly happen around the Low v Average and Average v High boundary. Pretty high accuracy. 


```{r}
class_rates <- numeric(100)

for(k in 1:100){
  Yhat_k <- knn(train, test, cl, k = k)
  
  conf_matx_k <- table(Predicted = Yhat_k, Actual = test_cl)
  
  class_rates[k] <- sum(diag(conf_matx_k)) / sum(conf_matx_k)
}
```

```{r}
best_k <- which.max(class_rates)
best_k
```
### k = 1 performed the best but this could be indicative that the model may be overfitting. 

```{r}
#Kennedy's comment
"I think there could be some potential overfitting of the training data implied by best_k = 1. The model might be learning the noise and specific details of the training set rather than the underlying generalizable patterns and i think this will probably lead to poor performance on new, unseen data. A k=1 model is highly sensitive to outliers

Even if the cross-validation accuracy was high with k=1, it's still susceptible to overfitting, especially if the dataset has noise or outliers. I have done previous projects with KNN before where I get low k values and high accuracy, when those two combine whether or not the model is acceptable will depend on the project's context."
```

```{r}
plot(1:100, class_rates, type = "b", pch = 19, col = "blue",
     xlab = "K Value", ylab = "Classification Accuracy")

abline(v = best_k, col = "red", lty = 3)
```






























































